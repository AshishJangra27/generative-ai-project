{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Build Your Own AI-Powered Article Search with Pinecone + Gemini**\n",
        "\n",
        "This notebook guides you through building a smart Q&A system that searches a large collection of technical articles and answers natural language questions using Google’s Gemini model.\n",
        "\n",
        "It combines:\n",
        "- **Semantic search** using vector embeddings + Pinecone  \n",
        "- **LLM-generated answers** based on retrieved context using Gemini\n",
        "\n",
        "---\n",
        "\n",
        "#### What You’ll Learn\n",
        "\n",
        "1. **Setup**\n",
        "   - Clone dataset and install required libraries  \n",
        "2. **Data Preparation**\n",
        "   - Merge article CSVs and convert to JSON  \n",
        "3. **Embedding Generation**\n",
        "   - Generate embeddings using Sentence Transformers or Qwen  \n",
        "4. **Pinecone Integration**\n",
        "   - Create an index and upload embeddings  \n",
        "5. **Question Answering**\n",
        "   - Query with a user question and get a Gemini-generated answer  \n",
        "\n",
        "---\n",
        "\n",
        "### Dataset Overview\n",
        "\n",
        "A curated set of technical articles from GeeksforGeeks, merged from multiple CSV files.\n",
        "\n",
        "- **Rows**: 49,328  \n",
        "- **Columns**:\n",
        "  - `title`: Topic/title of the article  \n",
        "  - `article`: Full content of the article  \n",
        "\n",
        "### Sample\n",
        "\n",
        "| Title                             | Article (snippet)                            |\n",
        "|----------------------------------|-----------------------------------------------|\n",
        "| What’s New in PHP 7 ?           | Prerequisite PHP 7 Features Set 1...          |\n",
        "| Kotlin Inheritance              | Kotlin supports inheritance which allows...   |\n",
        "| Merge two sorted linked list... | Merge two sorted linked list of size n1...    |\n",
        "\n",
        "> Articles cover topics like programming, web dev, data structures, and more—perfect for powering GenAI search tools.\n",
        "\n",
        "---\n",
        "\n",
        "By the end, you'll have a fully functional RAG system ready for real-world applications.\n",
        "\n",
        "\n",
        "### Download Preprocessed Data & Pretrained Embeddings\n",
        "\n",
        "To save time, you can directly download the **cleaned dataset** and **pretrained embeddings** from:\n",
        "\n",
        "- Hugging Face [(articles_dataset_for_rag)](https://huggingface.co/datasets/ashish-jangra/articles_dataset_for_RAG)  \n",
        "- Kaggle [(articles_dataset_for_rag)](https://www.kaggle.com/datasets/ashishjangra27/articles-dataset-for-rag)\n",
        "\n",
        "\n",
        "> These files can be loaded directly into the notebook to skip data processing and embedding generation steps.\n",
        "\n"
      ],
      "metadata": {
        "id": "vrOX_2dsx8fE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Data Preparation"
      ],
      "metadata": {
        "id": "KmuJNInq8UMD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.1) Get the data & install dependencies"
      ],
      "metadata": {
        "id": "CGjoZoRZr90M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AshishJangra27/datasets/\n",
        "!pip install pinecone"
      ],
      "metadata": {
        "id": "z2t2ybg4sEop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.2) Import Required Libraries"
      ],
      "metadata": {
        "id": "Z8cUMr1esFa5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import pinecone\n",
        "import unicodedata\n",
        "import pandas as pd\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from transformers import AutoModel, AutoTokenizer"
      ],
      "metadata": {
        "id": "S1ZH5HW2bmXR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.3) Setup APIs for PineCone and Gemini"
      ],
      "metadata": {
        "id": "fTWsug-ysOcf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PINECONE_API_KEY = \"pcsk_4zdnPr_9QmiNFJcSp6gmCYs1Xe49WhLVu51XUkovuooExLzcw3VVDkmrAaUAGKnPEx8YdM\"\n",
        "YOUR_GEMINI_API_KEY = \"\""
      ],
      "metadata": {
        "id": "_PEwJKN-sOwD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.4) Load and Combine the Datasets"
      ],
      "metadata": {
        "id": "x7Lgc4TLsVzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- List all CSV files from dataset folder  \n",
        "- Read and combine them into a single DataFrame  \n",
        "- Save the merged data as `data.csv`"
      ],
      "metadata": {
        "id": "U1ixAUBSu_aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csvs = [csv for csv in os.listdir('/content/datasets/GFG Articles/data') if '.csv' in csv]\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for csv in tqdm(csvs):\n",
        "    df_ = pd.read_csv('/content/datasets/GFG Articles/data/' + csv )\n",
        "    df = pd.concat((df,df_))\n",
        "\n",
        "df.to_csv('data.csv', index = False)"
      ],
      "metadata": {
        "id": "X_mc33oVsVGR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1.5) Data Formatting"
      ],
      "metadata": {
        "id": "enCgmaFUsnFE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Extract `title` and `article` columns  \n",
        "- Store as a list of dictionaries in `articles.json`  \n",
        "- Delete the original DataFrame to free memory  \n",
        "\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"id\": \"Understanding Linear Regression\",\n",
        "    \"text\": \"Linear regression is a fundamental algorithm in supervised learning...\"\n",
        "  },\n",
        "  {\n",
        "    \"id\": \"Introduction to Neural Networks\",\n",
        "    \"text\": \"Neural networks are inspired by the structure of the human brain...\"\n",
        "  }\n",
        "]\n"
      ],
      "metadata": {
        "id": "yrqbAW_kvHbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "articles = []\n",
        "for index, row in tqdm(df.iterrows()):\n",
        "    articles.append({\"id\": row['title'], \"text\": row['article']})\n",
        "\n",
        "with open('articles.json', 'w') as f:\n",
        "    json.dump(articles, f)\n",
        "\n",
        "del df"
      ],
      "metadata": {
        "id": "FYlt2_EDsUzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Generate Embeddings"
      ],
      "metadata": {
        "id": "UgPjUimf8vrV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1) Generate Embeddings with All_MiniLM"
      ],
      "metadata": {
        "id": "aAwjUG-kLuSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Set up device for GPU/CPU  \n",
        "- Load tokenizer and model from `sentence-transformers/all-MiniLM-L6-v2`  \n",
        "- Define helper functions:\n",
        "  - `to_ascii_id`: Converts text to ASCII-safe ID\n",
        "  - `get_embeddings`: Generates sentence embeddings using the transformer model  \n",
        "- Load `articles.json`  \n",
        "- Generate embeddings for each article  \n",
        "- Save results to `article_embeddings.json`  \n",
        "\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"id\": \"Understanding_Linear_Regression\",\n",
        "    \"embedding\": [0.0123, -0.0541, ..., 0.0998],\n",
        "    \"text\": \"Linear regression is a fundamental algorithm in supervised learning...\"\n",
        "  },\n",
        "  ...\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "DRAxrP-MvnKk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\").to(device)\n",
        "\n",
        "def to_ascii_id(text):\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "def get_embeddings(texts):\n",
        "\n",
        "    encoded_input = tokenizer(texts, padding=True, truncation=True, return_tensors='pt').to(device)\n",
        "    with torch.no_grad():\n",
        "        model_output = model(**encoded_input)\n",
        "\n",
        "    sentence_embeddings = (model_output.last_hidden_state * encoded_input['attention_mask'].unsqueeze(-1)).sum(dim=1) / encoded_input['attention_mask'].sum(dim=1, keepdim=True)\n",
        "    return sentence_embeddings.tolist()\n",
        "\n",
        "\n",
        "with open('articles.json', 'r') as file:\n",
        "    articles = json.load(file)\n",
        "\n",
        "embeddings_list = []\n",
        "\n",
        "for article in tqdm(articles, desc=\"Generating embeddings\"):\n",
        "    text = article[\"text\"]\n",
        "    try:\n",
        "\n",
        "        embedding = get_embeddings([text])[0]\n",
        "        embeddings_list.append({\"id\": to_ascii_id(article[\"id\"]), \"embedding\": embedding, \"text\": text}) # Include text here\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating embedding for article {article['id']}: {e}\")\n",
        "        embeddings_list.append({\"id\": to_ascii_id(article[\"id\"]), \"embedding\": None, \"text\": text}) # Include text here even if embedding fails\n",
        "\n",
        "\n",
        "print(f\"Generated embeddings for {len(embeddings_list)} articles.\")\n",
        "\n",
        "with open('article_embeddings.json', 'w') as f:\n",
        "    json.dump(embeddings_list, f)"
      ],
      "metadata": {
        "id": "SyEZJwIcgghx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2) Generate Embeddings with Qwen3 (**Optional**)"
      ],
      "metadata": {
        "id": "rE6r5L14L1XS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qwen3 .6B is a heavy model that will take hours around **15 hours** GPU to generate embeddings\n",
        "\n",
        "- Define `last_token_pool` to extract pooled embedding based on attention\n",
        "- Define `to_ascii_id` for ID normalization\n",
        "- Load tokenizer and model from `Qwen/Qwen3-Embedding-0.6B`\n",
        "- Set device (GPU/CPU)\n",
        "- Define `get_qwen_embeddings` to encode and normalize text embeddings\n",
        "- Load articles from `articles.json`\n",
        "- Generate embeddings and save as `article_embeddings_qwen.json`\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"id\": \"Understanding_Linear_Regression\",\n",
        "    \"embedding\": [0.0142, -0.0357, ..., 0.0871],\n",
        "    \"text\": \"Linear regression is a fundamental algorithm in supervised learning...\"\n",
        "  },\n",
        "  ...\n",
        "]\n",
        "```"
      ],
      "metadata": {
        "id": "XKSHuEkiv06z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def last_token_pool(last_hidden_states, attention_mask):\n",
        "    left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
        "    if left_padding:\n",
        "        return last_hidden_states[:, -1]\n",
        "    else:\n",
        "        seq_lens = attention_mask.sum(dim=1) - 1\n",
        "        return last_hidden_states[torch.arange(last_hidden_states.size(0)), seq_lens]\n",
        "\n",
        "\n",
        "def to_ascii_id(text):\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-Embedding-0.6B\", padding_side='left')\n",
        "model = AutoModel.from_pretrained(\"Qwen/Qwen3-Embedding-0.6B\").to(device)\n",
        "\n",
        "\n",
        "def get_qwen_embeddings(texts):\n",
        "    encoded = tokenizer(texts, padding=True, truncation=True, max_length=8192, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        output = model(**encoded)\n",
        "    pooled = last_token_pool(output.last_hidden_state, encoded['attention_mask'])\n",
        "    return F.normalize(pooled, p=2, dim=1).cpu().tolist()\n",
        "\n",
        "\n",
        "with open('articles.json', 'r') as f:\n",
        "    articles = json.load(f)\n",
        "\n",
        "results = []\n",
        "for article in tqdm(articles, desc=\"Generating Qwen embeddings\"):\n",
        "    text = article[\"text\"]\n",
        "    try:\n",
        "        emb = get_qwen_embeddings([text])[0]\n",
        "        results.append({\"id\": to_ascii_id(article[\"id\"]), \"embedding\": emb, \"text\": text})\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {article['id']}: {e}\")\n",
        "        results.append({\"id\": to_ascii_id(article[\"id\"]), \"embedding\": None, \"text\": text})\n",
        "\n",
        "with open('article_embeddings_qwen.json', 'w') as f:\n",
        "    json.dump(results, f)"
      ],
      "metadata": {
        "id": "FZLdE_YlLsQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Push Embeddings on Pinecone"
      ],
      "metadata": {
        "id": "ezasSq_etJD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.1) Initialize Index on Pinecone"
      ],
      "metadata": {
        "id": "4lKFkGEvH2rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Initialize Pinecone client using API key  \n",
        "- Set index name as `vector-db`  \n",
        "- Create index if it doesn't already exist  \n",
        "- Index configuration:\n",
        "  - **Dimension**: 384  \n",
        "  - **Metric**: Cosine similarity  \n",
        "  - **Cloud**: AWS  \n",
        "  - **Region**: us-east-1  \n",
        "- Print list of available indexes  "
      ],
      "metadata": {
        "id": "ewv4ksCawTRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pc = Pinecone(api_key=os.getenv(PINECONE_API_KEY))\n",
        "\n",
        "index_name = \"vector-db\"\n",
        "\n",
        "# if pc.has_index(index_name):\n",
        "#     pc.delete_index(name=index_name)\n",
        "\n",
        "if index_name not in [info[\"name\"] for info in pc.list_indexes()]:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\")\n",
        "    )\n",
        "\n",
        "print(pc.list_indexes())"
      ],
      "metadata": {
        "id": "a1Xv6turH1NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.2) Push Vectors on Pinecone"
      ],
      "metadata": {
        "id": "L9H8dO8PAHxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Initialize Pinecone client and connect to `vector-db` index  \n",
        "- Define utility functions:\n",
        "  - `to_ascii_id`: Ensures valid ASCII IDs  \n",
        "  - `truncate_text`: Limits metadata to max byte size  \n",
        "- Load `article_embeddings.json` and filter out entries with missing embeddings  \n",
        "- Batch upload to Pinecone with:\n",
        "  - **Batch size**: 50  \n",
        "  - **Max metadata**: 40,000 bytes  \n",
        "  - **Delay**: 1 second between batches  \n",
        "- Upload as: `(id, embedding, metadata)`  \n",
        "- Print total uploaded vector count  \n"
      ],
      "metadata": {
        "id": "pXwVrS1kwWzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(\"vector-db\")\n",
        "\n",
        "def to_ascii_id(text):\n",
        "    \"\"\"Convert text to ASCII-only string for Pinecone vector IDs.\"\"\"\n",
        "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "def truncate_text(text, max_bytes):\n",
        "    \"\"\"Truncate text to a maximum number of bytes, ensuring valid UTF-8.\"\"\"\n",
        "    truncated_text = text.encode('utf-8')[:max_bytes].decode('utf-8', 'ignore')\n",
        "    return truncated_text\n",
        "\n",
        "\n",
        "with open('article_embeddings.json', 'r') as file:\n",
        "    articles_with_embeddings = json.load(file)\n",
        "\n",
        "articles_with_embeddings = [article for article in articles_with_embeddings if article[\"embedding\"] is not None]\n",
        "\n",
        "BATCH_SIZE = 50\n",
        "DELAY = 1\n",
        "MAX_METADATA_BYTES = 40000\n",
        "\n",
        "total_batches = (len(articles_with_embeddings) + BATCH_SIZE - 1) // BATCH_SIZE\n",
        "\n",
        "for batch_start in tqdm(\n",
        "    range(0, len(articles_with_embeddings), BATCH_SIZE),\n",
        "    total=total_batches,\n",
        "    desc=\"Upserting batches\",\n",
        "    unit=\"batch\"\n",
        "):\n",
        "    batch = articles_with_embeddings[batch_start:batch_start + BATCH_SIZE]\n",
        "\n",
        "    vectors = []\n",
        "    for article in batch:\n",
        "        truncated_text = truncate_text(article[\"text\"], MAX_METADATA_BYTES)\n",
        "        vectors.append((article[\"id\"], article[\"embedding\"], {\"text\": truncated_text}))\n",
        "\n",
        "    index.upsert(vectors=vectors)\n",
        "\n",
        "    if batch_start + BATCH_SIZE < len(articles_with_embeddings):\n",
        "        time.sleep(DELAY)\n",
        "\n",
        "print(f\"Upserted {len(articles_with_embeddings)} article embeddings to vector-db\")"
      ],
      "metadata": {
        "id": "W7Zp1UrM5PBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Search for similar embedding Articles"
      ],
      "metadata": {
        "id": "SziZtPxDJGhK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.1) Search for similar embedding Articles"
      ],
      "metadata": {
        "id": "t8jrtSEnta_L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Initialize Pinecone client and access `vector-db`  \n",
        "- Embed the query using `get_embeddings`  \n",
        "- Search top 10 similar vectors using cosine similarity  \n",
        "- Print:\n",
        "  - Matched vector ID  \n",
        "  - Similarity score  \n",
        "  - First 200 characters of the matched article text  \n"
      ],
      "metadata": {
        "id": "Zm-hrTvbwsqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "index = pc.Index(\"vector-db\")\n",
        "\n",
        "query_text = \"what is gen-ai?\"\n",
        "query_embedding = get_embeddings([query_text])[0]\n",
        "\n",
        "response = index.query(\n",
        "    vector=query_embedding,\n",
        "    top_k=10,\n",
        "    include_metadata=True,\n",
        "    include_values=False)\n",
        "\n",
        "for match in response[\"matches\"]:\n",
        "    print(f\"ID: {match['id']}, score: {match['score']:.4f}\")\n",
        "    snippet = match[\"metadata\"].get(\"text\", \"\")\n",
        "    print(\"Snippet:\", snippet[:200].replace(\"\\n\", \" \"), \"...\\n\")"
      ],
      "metadata": {
        "id": "B2poF9VUJFmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.2) Get RAG based results with Gemini"
      ],
      "metadata": {
        "id": "LUy3tOjzUj2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Initialize Gemini client with API key  \n",
        "- Define `answer_with_gemini(query, top_k)` to:\n",
        "  - Embed the query  \n",
        "  - Retrieve top-k similar articles from Pinecone  \n",
        "  - Build a context from their metadata  \n",
        "  - Prompt Gemini model with context and question  \n",
        "- Print the model-generated answer  \n",
        "\n",
        "```python\n",
        "q = \"What is GAN?\"\n",
        "response = answer_with_gemini(q)\n",
        "print(\"Answer:\", response.text)\n",
        "```"
      ],
      "metadata": {
        "id": "LqqzbiUsw_Ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=YOUR_GEMINI_API_KEY)\n",
        "\n",
        "def answer_with_gemini(query, top_k=5):\n",
        "\n",
        "    q_emb = get_embeddings([query])[0]\n",
        "    resp = index.query(vector=q_emb, top_k=top_k, include_metadata=True)\n",
        "\n",
        "    context = \"\\n\\n\".join(\n",
        "        m[\"metadata\"].get(\"text\", \"\").strip()\n",
        "        for m in resp[\"matches\"]\n",
        "    )\n",
        "\n",
        "    response = client.models.generate_content(\n",
        "        model=\"gemini-1.5-flash\",\n",
        "        contents=[\n",
        "            types.Content(role=\"user\", parts=[types.Part(text=(\n",
        "                f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "            ))])\n",
        "        ],\n",
        "        config=types.GenerateContentConfig(\n",
        "            temperature=0.7,\n",
        "            max_output_tokens=500\n",
        "        )\n",
        "    )\n",
        "    return response\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    q = \"What is GAN?\"\n",
        "    response = answer_with_gemini(q)\n",
        "    print(\"Answer:\", response.text)"
      ],
      "metadata": {
        "id": "-wXHq50rTa2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4.3) Check Token Consumption"
      ],
      "metadata": {
        "id": "zeod_HMCZoP-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Print the Gemini model version used  \n",
        "- Extract and display the generated answer text  \n",
        "- Print token usage statistics:\n",
        "  - Prompt tokens  \n",
        "  - Completion tokens  \n",
        "  - Total tokens  "
      ],
      "metadata": {
        "id": "3VuouBtbxIyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model version:\", response.model_version)\n",
        "\n",
        "if response.candidates:\n",
        "    candidate = response.candidates[0]\n",
        "    text = \"\".join(part.text for part in candidate.content.parts)\n",
        "    print(\"\\nGenerated Text:\\n\", text.strip())\n",
        "\n",
        "\n",
        "usage = response.usage_metadata\n",
        "print(\"\\nTokens used in prompt:\", usage.prompt_token_count)\n",
        "print(\"Tokens in generated content:\", usage.candidates_token_count)\n",
        "print(\"Total tokens:\", usage.total_token_count)"
      ],
      "metadata": {
        "id": "Enw3-oIPVC-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Conclusion"
      ],
      "metadata": {
        "id": "vNtoP1J71K0B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we built a complete RAG-based system that combines semantic search using Pinecone with contextual answering using Gemini.\n",
        "\n",
        "From loading and embedding thousands of articles to querying with real questions and getting LLM-powered answers, you've seen how modern GenAI tools can work together to create powerful search experiences.\n",
        "\n",
        "#### What You Now Have:\n",
        "- A scalable, vector-based search backend using Pinecone  \n",
        "- Cleanly embedded article data using Transformer models  \n",
        "- A question-answering layer powered by Google Gemini  \n",
        "- A flexible pipeline you can extend with your own data or use cases\n",
        "\n",
        "> This setup can be the foundation for internal knowledge assistants, educational bots, or smart documentation search tools.\n",
        "\n",
        "Feel free to tweak, scale, and deploy it further — the core logic is now in your hands."
      ],
      "metadata": {
        "id": "dLPNqfUo1iXT"
      }
    }
  ]
}